% This LaTeX was auto-generated from MATLAB code.
% To make changes, update the MATLAB code and export to LaTeX again.

\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{epstopdf}
\usepackage[table]{xcolor}
\usepackage{matlab}

\sloppy
\epstopdfsetup{outdir=./}
\graphicspath{ {./Tutorial_images/} }

\begin{document}

\matlabtitle{Estimating context-dependent network in autoregressive point process models}

\matlabheading{Introduction}

\begin{par}
\begin{flushleft}
This is a tutorial for how to use the MATLAB functions in this folder to conduct estimation for context-dependent networks from time series data, based on autoregressive point process modeling. The methodologies are proposed in the paper \href{https://arxiv.org/abs/2003.07429}{Context-dependent self-exciting point processes: models, methods, and risk bounds in high dimensions}, including a multinomial approach which assumes a multinomial autoregressive model, a logistic-normal approach that assumes a logistic-normal autoregressive model, and a mixture approach assuming some nodes in the network follow the multinomial model while others follw the logistic-normal model. 
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
In this tutorial, we illustrate how to apply the multinomial, logistic-normal and mixture approaches through some examples.
\end{flushleft}
\end{par}

\matlabheading{Multinomial method}

\begin{par}
\begin{flushleft}
Assume that the time series data $\lbrace X^t \in R^{M\times K} \rbrace_{t=0}^T$ follow the multinomial autoregressive model defined in the paper, with network parameter $A^{MN} \in R^{M\times K\times M\times K}$ and offset parameter $\nu^{MN} \in R^{M\times K}$, then we can estimate$A^{MN}$ and $\nu^{MN}$ by applying the functions in the folder \textbf{multinomial.}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
\underline{\textit{\textbf{Generate data:}}}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
First we consider an example with $M=10$ nodes, and $K=2$ categories, each node is influenced by $\rho =1$ randomly chosen node. Each edge weight follows uniform distribution $U(-2,2)$, and the offset parameter is set to ensure the event probability to be \texttt{prob=0.8} when no past influence exists.
\end{flushleft}
\end{par}

\begin{matlabcode}
addpath('multinomial')
M=10;K=2;rho=1;
rng(2578);
A=zeros(M,K,M,K);
for i=1:M
    connect=randsample(M,rho);
    for k=1:size(connect)
     A(i,:,connect(k),:)=(rand(K,K)-0.5)*4;
    end
end
prob=0.8;
nu=ones(M,K)*log(prob/K/(1-prob));
\end{matlabcode}

\begin{par}
\begin{flushleft}
Then we generate multinomial time series data $\lbrace X^t \rbrace_{t=0}^T$ with sample size $T=1000$.
\end{flushleft}
\end{par}

\begin{matlabcode}
T=1000;
X=data_gen_MN(M,K,T,A,nu,prob);
\end{matlabcode}

\begin{par}
\begin{flushleft}
\underline{\textit{\textbf{Cross validation:}}}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Given data$\lbrace X^t \rbrace_{t=0}^T$, we can run 5-fold cross validation using function \texttt{cv\_MN.m}.\texttt{ }First we need to specify a list of tuning parameters for us to choose from. We can let the largest one $\lambda_{\max }$ to be the smallest $\lambda$ leading to zero estimate for $A^{MN}$, and then set the smallest one $\lambda_{\min }$ to be $0.001\times \lambda_{\max }$. We then choose the list of tuning parameters to be equally spaced between $\lambda_{\min }$ and $\lambda_{\max }$ under the log-scale.
\end{flushleft}
\end{par}

\begin{matlabcode}
init_nu=zeros(M,K);
init_A=zeros(M,K,M,K);
eta=2;tol=0.0001;iter=500;
\end{matlabcode}

\begin{par}
\begin{flushleft}
Choose $\lambda_{\max }$:
\end{flushleft}
\end{par}

\begin{matlabcode}
lambda_max=0.01;stop=false;
while ~stop
    lambda=lambda_max*K*sqrt(log(M)/T);
    [Ah,~,~,~,~,~]=fit_MN(X,lambda,true,init_A, init_nu, eta,tol,iter);
    if sum(Ah(:).^2)>0
        lambda_max=lambda_max*2;
    else
        stop=true;
    end
end
\end{matlabcode}

\begin{par}
\begin{flushleft}
Choose $\lambda_{\min }$:
\end{flushleft}
\end{par}

\begin{matlabcode}
lambda_min=lambda_max*0.001;
\end{matlabcode}

\begin{par}
\begin{flushleft}
Generate evenly spaced 20 tuning parameters under the log-scale:
\end{flushleft}
\end{par}

\begin{matlabcode}
lambda_c_list=exp(linspace(log(lambda_min),log(lambda_max),20));
\end{matlabcode}

\begin{par}
\begin{flushleft}
Given the list of tuning parameters, we run 5-fold cross validation:
\end{flushleft}
\end{par}

\begin{matlabcode}
pred_err=zeros(1,length(lambda_c_list));pred_loss=zeros(1,length(lambda_c_list));
for i=1:length(lambda_c_list)
    output=cv_MN(X,lambda_c_list(i),true,init_A,init_nu,eta,tol,iter);
    pred_loss(i)=mean(cell2mat(output.pred_loss));
    pred_err(i)=mean(cell2mat(output.pred_err));
end  
\end{matlabcode}

\begin{par}
\begin{flushleft}
\texttt{pred\_loss} and \texttt{pred\_err }contain the average log-likelihood loss and prediction errors evaluated at the test sets in 5-fold cross validation, and either one can be used as the criterion for choosing lambda.
\end{flushleft}
\end{par}

\begin{matlabcode}
[~,I]=min(pred_loss);
\end{matlabcode}

\begin{par}
\begin{flushleft}
\underline{\textit{\textbf{Estimate the parameters:}}}
\end{flushleft}
\end{par}

\begin{matlabcode}
lambda=lambda_c_list(I)*K*sqrt(log(M)/T);
[Ah,nu_h,~,~,~,~]=fit_MN(X,lambda,true,init_A, init_nu, eta,tol,iter);
\end{matlabcode}

\begin{par}
\begin{flushleft}
Estimation error:
\end{flushleft}
\end{par}

\begin{matlabcode}
sqrt(sum((Ah(:)-A(:)).^2))
\end{matlabcode}
\begin{matlaboutput}
ans = 2.1755
\end{matlaboutput}
\begin{matlabcode}
sqrt(sum((nu_h(:)-nu(:)).^2))
\end{matlabcode}
\begin{matlaboutput}
ans = 1.0627
\end{matlaboutput}

\matlabheading{Logistic-normal method}

\begin{par}
\begin{flushleft}
Assume that the time series data $\lbrace X^t \in R^{M\times K} \rbrace_{t=0}^T$ follow the logistic-normal autoregressive model with event probability varying over time, where the network parameters are$A^{LN} \in R^{M\times (K-1)\times M\times K}$, $B^{Bern} \in R^{M\times 1\times M\times K}$ and offset parameters are $\nu^{LN} \in R^{M\times (K-1)}$, $\eta^{Bern} \in R^{M\times 1}$. We can estimate these parameters by applying the functions in the folder \textbf{logistic-normal-varying-q. }For simplicity of the codes, we will use only two parameters \texttt{A }and \texttt{nu}, where \texttt{A} is concatenated by $A^{LN}$ and $B^{Bern}$ in the second dimension, \texttt{nu} is concatenated by $\nu^{LN}$ and $\eta^{Bern}$in the second dimension. When assuming the event probability for each node is a constant ($B^{Bern} =0$), the functions in folder \textbf{logistic-normal-constant-q }can be used, while for presentation simplicity we will omit this special case.
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
\underline{\textit{\textbf{Generate data:}}}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Consider the same setting as the multinomial example above, and the standard deviation for the Gaussian noise in logistic-normal model is set as $\sigma =1$.
\end{flushleft}
\end{par}

\begin{matlabcode}
addpath('logistic-normal-varying-q')
M=10;K=2;rho=1;
rng(2578);
A=zeros(M,K,M,K);
for i=1:M
    connect=randsample(M,rho);
    for k=1:size(connect)
     A(i,:,connect(k),:)=(rand(K,K)-0.5)*4;
    end
end
prob=0.8;
nu=zeros(M,K);nu(:,K)=ones(M,1)*log(prob/(1-prob));sigma=1;
\end{matlabcode}

\begin{par}
\begin{flushleft}
Then we generate logistic-normal time series data $\lbrace X^t \rbrace_{t=0}^T$ with sample size $T=1000$.
\end{flushleft}
\end{par}

\begin{matlabcode}
T=1000;
X=data_gen_LN_Bern(M,K,T,A,nu,prob,sigma);
\end{matlabcode}

\begin{par}
\begin{flushleft}
\underline{\textit{\textbf{Cross validation:}}}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Specify the list of tuning parameters to choose from, including a list of $\alpha$ and a list of $\lambda$.
\end{flushleft}
\end{par}

\begin{matlabcode}
init_nu=zeros(M,K);
init_A=zeros(M,K,M,K);
eta=2;tol=0.0001;iter=500;
\end{matlabcode}

\begin{par}
\begin{flushleft}
Choose $\lambda_{\max }$:
\end{flushleft}
\end{par}

\begin{matlabcode}
lambda_max=0.01;stop=false;alpha_list=[0.3 0.5 0.7];
for i=1:length(alpha_list)
    while ~stop
        lambda=lambda_max*K*sqrt(log(M)/T);
        [Ah,nu_h,~,~,~,~]=fit_LN_Bern(X,lambda,alpha_list(i),true,init_A, init_nu, eta,tol,iter);
        if sum(Ah(:).^2)>0
            lambda_max=lambda_max*2;
            init_A=Ah;init_nu=nu_h;
        else
            stop=true;
        end
    end
end
\end{matlabcode}

\begin{par}
\begin{flushleft}
Choose $\lambda_{\min }$:
\end{flushleft}
\end{par}

\begin{matlabcode}
lambda_min=lambda_max*0.001;
\end{matlabcode}

\begin{par}
\begin{flushleft}
Generate evenly spaced 20 tuning parameters under the log-scale:
\end{flushleft}
\end{par}

\begin{matlabcode}
lambda_c_list=exp(linspace(log(lambda_min),log(lambda_max),20));
\end{matlabcode}

\begin{par}
\begin{flushleft}
Perform 5-fold cross validation to choose $\alpha$ and $\lambda$.
\end{flushleft}
\end{par}

\begin{matlabcode}
pred_err=zeros(length(alpha_list),length(lambda_c_list));
for i=1:length(alpha_list)
    for j=1:length(lambda_c_list)
        output=cv_LN_Bern(X,lambda_c_list(j),alpha_list(i),true,init_A,init_nu,eta,tol,iter);
        pred_err(i,j)=mean(cell2mat(output.pred_err));%
    end
end
[I,J]=find(pred_err==min(pred_err(:)));alpha=alpha_list(I);lambda=lambda_c_list(J)*K*sqrt(log(M)/T);
\end{matlabcode}

\begin{par}
\begin{flushleft}
\underline{\textit{\textbf{Estimate the parameters:}}}
\end{flushleft}
\end{par}

\begin{matlabcode}
[Ah,nu_h,~,~,~,~]=fit_LN_Bern(X,lambda,alpha,true,init_A, init_nu, eta,tol,iter);
\end{matlabcode}

\begin{par}
\begin{flushleft}
Estimation error:
\end{flushleft}
\end{par}

\begin{matlabcode}
sqrt(sum((Ah(:)-A(:)).^2))
\end{matlabcode}
\begin{matlaboutput}
ans = 2.5604
\end{matlaboutput}
\begin{matlabcode}
sqrt(sum((nu_h(:)-nu(:)).^2))
\end{matlabcode}
\begin{matlaboutput}
ans = 0.8027
\end{matlaboutput}

\matlabheading{Mixture method}

\begin{par}
\begin{flushleft}
Assume that some nodes follow the multinomial autoregressive model while others follow the logistic-normal autoregressive model with time-varying event probability. Furthermore, we assume that the true categories of events associated with multinomial nodes are not accurately observed, but contaminated to be logistic-normal random vectors lying on a simplex. This is the setting of the synthetic toy model in our paper that mimics the real data behavior, and more details are included in the paper. We will show in the following how to identify the type of each node (multinomial or logistic-normal), and how to estimate the network parameter based on the their types. The required functions iare included n the folder \textbf{mixture.}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
\underline{\textit{\textbf{Generate data:}}}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Consider the same setting as the two examples above, with first 5 nodes being logistic-normal nodes (\texttt{LN\_list}) and last 5 nodes (\texttt{MN\_list}) being multinomial nodes.
\end{flushleft}
\end{par}

\begin{matlabcode}
addpath('mixture')
M=10;K=2;rho=1;
rng(2578);
A_mix=zeros(M,K,M,K);
for i=1:M
    connect=randsample(M,rho);
    for k=1:size(connect)
     A_mix(i,:,connect(k),:)=(rand(K,K)-0.5)*4;
    end
end
LN_list=1:5;MN_list=6:10;
nu_mix=zeros(M,K);prob=0.8;
nu_mix(MN_list,:)=ones(length(MN_list),K)*log(prob/K/(1-prob));
\end{matlabcode}

\begin{par}
\begin{flushleft}
First we generate the clean mixture data with sample size $T=1000$.
\end{flushleft}
\end{par}

\begin{matlabcode}
T=1000;sigma_LN=1;
X=data_gen_mixed(M,K,T,A_mix,nu_mix,prob,sigma_LN,LN_list);
\end{matlabcode}

\begin{par}
\begin{flushleft}
Then we contaminate the data associated to multinomial nodes to be logistic-normally distributed random vectors.
\end{flushleft}
\end{par}

\begin{matlabcode}
a=1;sigma_MN=0.3;
X_contam=contaminate_MN_nodes(X,MN_list,a,sigma_MN);
\end{matlabcode}

\begin{par}
\begin{flushleft}
\underline{\textit{\textbf{Identify the type of each node:}}}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Given the contaminated data \texttt{X\_contam}, we use the function \texttt{testing.m} to identify the type of each node, which outputs a test statistic \texttt{logLR}. Since we need to fit two models based on the contaminated data in \texttt{testing.m}, cross validation is also needed. The input \texttt{nlambda} is the number of potential tuning parameters to conider in cross validation. We can then estimate the set of logistic-normal nodes to be the nodes with test statistic \texttt{logLR }equal to \texttt{-Inf.}
\end{flushleft}
\end{par}

\begin{matlabcode}
nlambda=20;
[logLR,~,~,~,~,~,~]=testing(X_contam,nlambda);
LN_list_hat=find(logLR==-Inf);MN_list_hat=find(logLR~=-Inf);
\end{matlabcode}

\begin{par}
\begin{flushleft}
Given the estimated node types, we can preprocess the contaminated data by rounding the event data associated with multinomial nodes.
\end{flushleft}
\end{par}

\begin{matlabcode}
X_mix=X_contam;
for m=1:length(MN_list_hat)
    for t=1:(T+1)
        if max(X_contam(t,m,:))>0
           [~,I]=sort(X_contam(t,m,:),'descend');
           X_mix(t,m,I(1))=1;
        end
    end
end
\end{matlabcode}

\begin{par}
\begin{flushleft}
\underline{\textit{\textbf{Cross validation for fitting the mixture model:}}}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
To fit the mixture model based on preprocesse data \texttt{X\_mix,} three tuning parameters are required: $\lambda^{LN}$, $\alpha$ and $\lambda^{MN}$. The first two are determined by cross validation on the logistic-normal nodes, while the last one is determinied by cross validation on the multinomial node.
\end{flushleft}
\end{par}

\begin{matlabcode}
init_nu=zeros(M,K);
init_A=zeros(M,K,M,K);
eta=2;tol=0.0001;iter=500;
\end{matlabcode}

\begin{par}
\begin{flushleft}
Cross validation for logistic-normal nodes:
\end{flushleft}
\end{par}

\begin{matlabcode}
lambda_c_LN_max=0.01;stop=false;alpha_list=[0.3 0.5 0.7];
for i=1:length(alpha_list)
    while ~stop
        lambda=lambda_c_LN_max*K*sqrt(log(M)/T);
        Ah=zeros(M,K,M,K);nu_h=zeros(M,K);
        for m=LN_list_hat
            [Ah(m,:,:,:),nu_h(m,:),~,~,~,~]=fit_LN_node(X_mix(1:T,:,:),X_mix(2:(T+1),m,:),lambda,alpha_list(i),true,init_A(m,:,:,:), init_nu(m,:), eta,tol,iter);
        end
        if sum(Ah(:).^2)>0
            lambda_c_LN_max=lambda_c_LN_max*2;
            init_A=Ah;init_nu=nu_h;
        else
            stop=true;
        end
    end
end
lambda_c_LN_min=lambda_c_LN_max*0.001;
lambda_c_LN_list=exp(linspace(log(lambda_c_LN_min),log(lambda_c_LN_max),20));
pred_err_LN=zeros(length(LN_list_hat),length(alpha_list),length(lambda_c_LN_list));
for i=1:length(LN_list_hat)
    m=LN_list_hat(i);
    for j1=1:length(alpha_list)
        for j2=1:length(lambda_c_LN_list)
            output=cv_LN_node(X_mix(1:T,:,:),X_mix(2:(T+1),m,:),lambda_c_LN_list(j2),alpha_list(j1),true,init_A(m,:,:,:),init_nu(m,:),eta,tol,iter);
            pred_err_LN(i,j1,j2)=mean(cell2mat(output.pred_err));  
            Ah_temp=zeros(1,K,M,K);nu_h_temp=zeros(1,K);
            for k=1:5
                Ah_temp(1,:,:,:)=Ah_temp(1,:,:,:)+output.Ah{k};
                nu_h_temp(1,:)=nu_h_temp(1,:)+output.nu_h{k};
            end
            init_A(m,:,:,:)=Ah_temp/5;init_nu(m,:)=nu_h_temp/5;
        end
    end
end
pred_err_LN=reshape(mean(pred_err_LN,1),length(alpha_list),length(lambda_c_LN_list));
[I,J]=find(pred_err_LN==min(pred_err_LN(:)));
LN_alpha=alpha_list(I);LN_lambda_c=lambda_c_LN_list(J);
\end{matlabcode}

\begin{par}
\begin{flushleft}
Cross validation for MN nodes
\end{flushleft}
\end{par}

\begin{matlabcode}
lambda_c_MN_max=0.01;stop=false;
while ~stop
    lambda=lambda_c_MN_max*K*sqrt(log(M)/T);
    Ah=zeros(M,K,M,K);nu_h=zeros(M,K);
    for m=MN_list_hat
        [Ah(m,:,:,:),nu_h(m,:),~,~,~,~]=fit_MN_node(X_mix(1:T,:,:),X_mix(2:(T+1),m,:),lambda,true,init_A(m,:,:,:), init_nu(m,:), eta,tol,iter);
    end
    if sum(Ah(:).^2)>0
        lambda_c_MN_max=lambda_c_MN_max*2;
        init_A=Ah;init_nu=nu_h;
    else
        stop=true;
    end
end
lambda_c_MN_min=lambda_c_MN_max*0.001;
lambda_c_MN_list=exp(linspace(log(lambda_c_MN_min),log(lambda_c_MN_max),20));
pred_err_MN=zeros(length(MN_list_hat),length(lambda_c_MN_list));
for i=1:length(MN_list_hat)
    m=MN_list_hat(i);
    for j=1:length(lambda_c_MN_list)
        output=cv_MN_node(X_mix(1:T,:,:),X_mix(2:(T+1),m,:),lambda_c_MN_list(j),true,init_A(m,:,:,:),init_nu(m,:),eta,tol,iter);
        pred_err_MN(i,j)=mean(cell2mat(output.pct_err));  
        Ah_temp=zeros(1,K,M,K);nu_h_temp=zeros(1,K);
        for k=1:5
            Ah_temp(1,:,:,:)=Ah_temp(1,:,:,:)+output.Ah{k};
            nu_h_temp(1,:)=nu_h_temp(1,:)+output.nu_h{k};
        end
        init_A(m,:,:,:)=Ah_temp/5;init_nu(m,:)=nu_h_temp/5;
    end
end
[~,ind]=min(mean(pred_err_MN,1)); 
MN_lambda_c=lambda_c_MN_list(ind);
\end{matlabcode}

\begin{par}
\begin{flushleft}
\underline{\textit{\textbf{Estimate the parameters:}}}
\end{flushleft}
\end{par}

\begin{matlabcode}
LN_lambda=LN_lambda_c*K*sqrt(log(M)/T);
MN_lambda=MN_lambda_c*K*sqrt(log(M)/T);
Ah_mix=zeros(M,K,M,K);nu_h_mix=zeros(M,K);
init_A=zeros(M,K,M,K);init_nu=zeros(M,K);
for m=1:M
    if sum(LN_list_hat==m)>0
        [Ah_mix(m,:,:,:),nu_h_mix(m,:),~,~,~,~]=fit_LN_node(X_mix(1:T,:,:),X_mix(2:(T+1),m,:),LN_lambda,LN_alpha,true,init_A(m,:,:,:), init_nu(m,:), eta,tol,iter);
    else
        [Ah_mix(m,:,:,:),nu_h_mix(m,:),~,~,~,~]=fit_MN_node(X_mix(1:T,:,:),X_mix(2:(T+1),m,:),MN_lambda,true,init_A(m,:,:,:), init_nu(m,:), eta,tol,iter);
    end
end
\end{matlabcode}

\begin{par}
\begin{flushleft}
Estimation error:
\end{flushleft}
\end{par}

\begin{matlabcode}
sqrt(sum((Ah_mix(:)-A_mix(:)).^2))
\end{matlabcode}
\begin{matlaboutput}
ans = 5.0571
\end{matlaboutput}
\begin{matlabcode}
sqrt(sum((nu_h_mix(:)-nu_mix(:)).^2))
\end{matlabcode}
\begin{matlaboutput}
ans = 0.9574
\end{matlaboutput}

\vspace{1em}

\end{document}
